{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMmxXlb1qrgy"
   },
   "source": [
    "# TensorFlow Basics\n",
    "\n",
    "There is a vast amount of material freely available on deep learning in general, and TensorFlow in particular. Although we introduce several useful techniques in this notebook, to become very proficient in TensorFlow you will have to explore further on your own. \n",
    "\n",
    "Useful links to get you started, include:\n",
    "\n",
    "1. A good place to start: https://www.tensorflow.org/get_started/get_started\n",
    "2. Various other tutorials are here: https://www.tensorflow.org/versions/r0.10/tutorials/index.html\n",
    "3. Aurélien Géron’s set of notebooks to complement his equally informative book, Hands-On Machine Learning with Scikit-Learn & TensorFlow.\n",
    "\n",
    "\n",
    "Deep neural networks often involve a huge number of trainable parameters, easily of the order of tens of millions. This large number of parameters requires a vast amount of training data, and the computational resources to handle all of this. TensorFlow is designed for exactly this purpose, as described in the 2015 whitepaper http://download.tensorflow.org/paper/whitepaper2015.pdf. \n",
    "\n",
    "One of the important design issues is that of scalability. TensorFlow should be able to take advantage of whatever computational power is available, whether it is the CPU on your cell phone or laptop, multiple GPU’s, or distributed machines. To allow this, TensorFlow has a very different design than more traditional programming languages such as Python, Java and C++, among others. In NumPy, for example, the different methods (like SVD, EIG, etc.) reside outside Python where compiled code is run. If not, everything will be slow. In TensorFlow, everything is run outside Python (assuming you use the Python API as we will do in these tutorials).  \n",
    "\n",
    "To achieve this, and also to take advantage of the available computational power, TensorFlow computations proceed in two steps. First, a computational graph is constructed. The nodes represent abstract operations, such as matrix multiplication or function evaluation. Each node receives zero or more data-inputs, and zero or more outputs. The data, in the form of tensors, flow along the edges. During this first stage, the computational graph is constructed symbolically. It is important to keep in mind that, at this stage, no computations whatsoever are performed. In fact, at this point, the graph is empty – that is, it only exists symbolically. The second execution stage is when the graph is populated with values and when the operations are executed. This happens by creating a session. \n",
    "\n",
    "It is perhaps best to illustrate this with a very simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4VWwiZHqrgz"
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import IPython.display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plt.rcParams['axes.facecolor'] = \"0.92\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEZ-hYa-qrg2"
   },
   "source": [
    "### Let's start by initialising two contants, called matrix1 and matrix 2\n",
    "\n",
    "Note that you cannot do anything with these constants. They need to be launched in a **session** first. You can't even print their values. Try for example to run **matrix1.eval** \n",
    "\n",
    "TensorFlow constants are exactly what their name implies: they are assigned values, and cannot be changed.\n",
    "\n",
    "It is always a good idea to reset the default graph. If not, a second execution of the cell will add to the previously constructed, and still existing graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rp2hlIxsqrg3"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fwexkM-qrg5"
   },
   "source": [
    "Let's look at the constants. It is not quite what you would expect because at this point they are simply nodes in the computational graph. We'll get hold of their values below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHrZNPTYqrg5"
   },
   "outputs": [],
   "source": [
    "print('matrix1:', matrix1, 'matrix2', matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYMT-8AVqrg-"
   },
   "source": [
    "### Create a matrix multiplication op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpmbfSAkqrg-"
   },
   "outputs": [],
   "source": [
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2vMB_2nqrhB"
   },
   "source": [
    "### Launch graph in a session\n",
    "\n",
    "This constructs the computational graph so that you can do something with it. If you launch a session, resources such as the CPU or GPU are automatically allocated to the session. You can also do this manually if, for example, you have more than one GPU, or you want to do distributed computing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAIGZp5HqrhC"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7c7Wh3wqrhF"
   },
   "source": [
    "After the session is closed, all TensorFlow tensors and ops cease to exist. What about the **result** computed above? Let's investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4iRJAAOqrhG"
   },
   "outputs": [],
   "source": [
    "print(result)\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05p2jEchqrhK"
   },
   "source": [
    "**result** is actually a NumPy array and therefore persists after the session is closed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K--6iYlyqrhL"
   },
   "source": [
    "### Note:\n",
    "* All ops lower down in the graph – i.e. the dependencies of **product** in the case the constant ops, **matrix1** and **matrix2**, are first executed.\n",
    "* matmul is matrix multiplication – not for example,  pointwise multiplication\n",
    "* It is important to close the session when you are done in order to save resources. (We used the \"with funciton\" to do this)\n",
    "* Better to use interactive sessions if you work in iPython or Jupyter.\n",
    "* Close the session before you run the cell a second time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Limvg5RvqrhM"
   },
   "source": [
    "Let's find the values of the constants we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-4Hq5kgqrhN"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('matrix1:', matrix1.eval(),'matrix2', matrix2.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jl7raN1gqrhQ"
   },
   "source": [
    "Here is another way of starting a session. This is particularly useful for interactive sessions, as we use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_062SSimqrhS"
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "# Initialize 'x' using the run() method of its initializer op.\n",
    "x.initializer.run()\n",
    "\n",
    "# Add an op to subtract 'a' from 'x'.  Run it and print the result\n",
    "sub = x - a\n",
    "print(sub.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5OpeGEFAqrhV"
   },
   "source": [
    "### Variables and placeholders\n",
    "\n",
    "The next important data structures are the *variable* and *placeholder*. You can think of the *variables* as your trainable parameters. In fact, by declaring something a variable, TensorFlow's **tf.gradients()** method will automatically calculate the gradient of a cost function with respect to all the specified variables.\n",
    "\n",
    "All variables need to be explicitly initialized. Also, since variables are trainable parameters, they are assigned initial values during declaration.\n",
    "\n",
    "**Placeholder**s are, as the name implied, waiting for values to be assigned to them during execution. Note that **tf.gradients()** ignore placeholders. Typically, you will feed your training or test *data* into placeholders during execution, using a dictionary feed. \n",
    "\n",
    "Read more on the subject here: https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html\n",
    "\n",
    "**Note:** \n",
    "\n",
    "There are multiple ways of launching sessions. We provide one important example: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huulDKvgqrhW"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a Variable, that will be initialized to the scalar value 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# Create an update Op.\n",
    "update = tf.assign(state, state + 1)\n",
    "\n",
    "# Variables must be initialized by running an `init` Op after having\n",
    "# launched the graph.  We first have to add the `init` Op to the graph.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph and run the ops.\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "  # Run the 'init' op. \n",
    "  sess.run(init_op)\n",
    "    \n",
    "  # Print the initial value of 'state'\n",
    "  print(state.eval())\n",
    "    \n",
    "  # Run the op that updates 'state' and print 'state'. Note that the graph is executed several times.\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(state.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-tBru7Nqrhc"
   },
   "source": [
    "Note that all the tensors are cleared when the session is closed. So, how do we get hold of a variable outside the session? One has to assign it to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLP-yBQKqrhe"
   },
   "outputs": [],
   "source": [
    "# Note that this will fail, as expected\n",
    "print(state.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKayyKG6qrhg"
   },
   "source": [
    "Let's assign it to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O07RVNaiqrhh"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a Variable, that will be initialized to the scalar value 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# Create an Op to add one to `state`.\n",
    "update = tf.assign(state, state + 1)\n",
    "\n",
    "# Create the init Op\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph and run the Ops.\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run the 'init' op. \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Print the initial value of 'state' in two different ways\n",
    "    print(sess.run(state))\n",
    "  \n",
    "    # Run the op that updates 'state' and print 'state'. Note that the graph is executed several times.\n",
    "    for _ in range(3):        \n",
    "        sess.run(update)\n",
    "        print(state.eval())\n",
    "        \n",
    "    val = state.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Slibyeikqrhk"
   },
   "outputs": [],
   "source": [
    "# Now we have extracted the final value of state\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdtEHeGFqrho"
   },
   "source": [
    "### Multiple outputs\n",
    "\n",
    "Below is an example of how you can fetch multiple tensors at the same time. The values (NumPy arrays) of the different tensors are returned in a list. \n",
    "\n",
    "Note that the graph is executed only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gK33Fjuqrho"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Declarations\n",
    "input1 = tf.constant([3.0])\n",
    "input2 = tf.constant([2.0])\n",
    "input3 = tf.constant([5.0])\n",
    "\n",
    "# Calculations\n",
    "intermed = tf.add(input2, input3)\n",
    "mul = input1 * intermed\n",
    "\n",
    "# Run session\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([mul, intermed])\n",
    "  print(result)\n",
    "  print(result[1])\n",
    "  print(result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDROb9hxqrhv"
   },
   "source": [
    "# Placeholders and feed\n",
    "\n",
    "When creating placeholders, you can feed them values when you execute the graph. \n",
    "\n",
    "The placeholders are *fed* through dictionaries.\n",
    "\n",
    "Note that you input arrays and the outputs are also arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn_urRRzqrhw"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Declarations\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "\n",
    "# Calculations\n",
    "output = input1 * input2\n",
    "\n",
    "# Run session\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(output, feed_dict={input1: 7., input2: 2. }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIksCtPkqrh0"
   },
   "source": [
    "## Linear regression example\n",
    "\n",
    "In this case the model is of the form, $$ y = w x + b $$\n",
    "and the idea is to estimate $w$ and $b$ given $(x,y)$ pairs.\n",
    "\n",
    "Although one can solve the system using the normal equations, we are going to use TensorFlow's built-in gradient calculator to solve the system using gradient descent. \n",
    "\n",
    "Let’s first generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yui-2tyCqrh2"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x_train = np.linspace(0, 1, 100)\n",
    "y_train = 0.2 * x_train + 1 + 0.01 * np.random.randn(x_train.shape[0])\n",
    "\n",
    "plt.plot(x_train, y_train, 'r.')\n",
    "plt.title('Generated Data for Regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XY9utNyuqrh6"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Set hyperparameter\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters (variables). Initialised with values not close to the correct values.\n",
    "w = tf.Variable([-3.], dtype=tf.float32)\n",
    "b = tf.Variable([3.], dtype=tf.float32)\n",
    "\n",
    "# Model input (placeholders)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Linear model\n",
    "linear_model = w * x + b\n",
    "\n",
    "# loss\n",
    "mse = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "\n",
    "#Gradient\n",
    "grad = tf.gradients(mse, [w, b])\n",
    "\n",
    "# Gradient descent update\n",
    "update_w = tf.assign(w, w - learning_rate * grad[0])\n",
    "update_b = tf.assign(b, b - learning_rate * grad[1])\n",
    "\n",
    "# Define init Op\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Descend for 1000 steps\n",
    "    for i in range(1000):\n",
    "        # print loss at every 100th step\n",
    "        if i%100 == 0:\n",
    "            print('mse = ',sess.run(mse, feed_dict = {x:x_train, y:y_train}))\n",
    "            \n",
    "        sess.run([update_w, update_b], {x:x_train, y:y_train})\n",
    "\n",
    "    # evaluate training accuracy\n",
    "    curr_W, curr_b, curr_mse = sess.run([w, b, mse], {x:x_train, y:y_train})\n",
    "    print(\"w: %s b: %s loss: %s\"%(curr_W, curr_b, curr_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awz6um8Gqrh9"
   },
   "source": [
    "As you can see, the results above are very close to 0.2 W and 1 b that we set. The reason for the slight difference is the error that we introduced: **0.01*np.random.randn(x_train.shape[0])**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "soAdvm_iqrh-"
   },
   "source": [
    "Experiment with the learning rate and the number of training steps and see how that affects your accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxxQJgKQqrh_"
   },
   "source": [
    "## Let's use TensorFlow's built-in optimiser\n",
    "\n",
    "Again, we first reset the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ns9bemD9qrh_"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Variables\n",
    "w = tf.Variable([-0.3], dtype=tf.float32)\n",
    "b = tf.Variable([0.3], dtype=tf.float32)\n",
    "\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Linear model\n",
    "linear_model = w * x + b\n",
    "\n",
    "# loss\n",
    "mse = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = optimizer.minimize(mse)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Execute\n",
    "with tf.Session() as sess:\n",
    "     \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # 1000 steps\n",
    "    for i in range(1000):\n",
    "        # Print loss every 100 steps\n",
    "        if i%100 == 0:\n",
    "            print('mse = ', sess.run(mse, feed_dict = {x:x_train, y:y_train}))\n",
    "        \n",
    "        sess.run(train_step, feed_dict = {x:x_train,y:y_train})\n",
    "    \n",
    "    # Print final results\n",
    "    print('w : ',w.eval(),'b : ', b.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YxaIWlJEqriB"
   },
   "source": [
    "# Visualisation using TensorBoard\n",
    "\n",
    "TensorFlow provides a convenient utility of visualizing the computation graph, called TensorBoard. Computation graphs quickly become quite complicated, and this is reflected in the computation graphs. It is, therefore, important to effectively visualize the computation graph. One of the convenient mechanisms is the **tf.name_scope( )**. This allows one to group tensors that belong together under the same named scope. This is something you probably want to do anyway, to organize your code, regardless of whether you are going to use the visualizations.\n",
    "\n",
    "Apart from the visualization, TensorBoard also allows one to visualize how quantities – such as the loss function – change during training.\n",
    "\n",
    "### Let's do this systematically, and first just add the name scopes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgM9IoEAqriD"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters (variables). Now organised under the same named scope\n",
    "# It is important to name the variables; this is what will be used in tensorboard\n",
    "with tf.name_scope('variables') as scope:\n",
    "    w = tf.Variable([-3.], dtype=tf.float32, name = 'w')\n",
    "    b = tf.Variable([3.], dtype=tf.float32, name = 'b')\n",
    "\n",
    "# Model input (placeholders)\n",
    "with tf.name_scope('placeholders') as scope:\n",
    "    x = tf.placeholder(tf.float32, name = 'x')\n",
    "    y = tf.placeholder(tf.float32, name = 'y')\n",
    "\n",
    "# Linear model\n",
    "with tf.name_scope('model') as scope:\n",
    "    linear_model = w * x + b\n",
    "    mse = tf.reduce_sum(tf.square(linear_model - y), name = 'mse') \n",
    "    grad = tf.gradients(mse, [w, b], name = 'grad')\n",
    "\n",
    "# Gradient descent update\n",
    "with tf.name_scope('training') as scope:\n",
    "    update_w = tf.assign(w, w - learning_rate * grad[0], name = 'update_w')\n",
    "    update_b = tf.assign(b, b - learning_rate * grad[1], name = 'update_b')\n",
    "\n",
    "# Define init Op\n",
    "with tf.name_scope('init_op') as scope:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    # In order to see how the naming convention has changes, let's print the variable names\n",
    "    var = [v.name for v in tf.trainable_variables()]\n",
    "    print(var)\n",
    "    \n",
    "    # 1000 steps\n",
    "    for i in range(1000):\n",
    "        # Report loss every 100 steps\n",
    "        if i%100 == 0:\n",
    "            print('mse = ',sess.run(mse, feed_dict = {x:x_train, y:y_train} ))\n",
    "        \n",
    "        sess.run([update_w, update_b], {x:x_train, y:y_train})\n",
    "\n",
    "    # evaluate training accuracy\n",
    "    final_w, final_b, final_mse = sess.run([w, b, mse], {x:x_train, y:y_train})\n",
    "    print(\"w: %s b: %s mse: %s\"%(final_w, final_b, final_mse))\n",
    "    \n",
    "    # One can also retrieve the trained values of the variables directly from the graph.\n",
    "    w = graph.get_tensor_by_name(\"variables/w:0\").eval()\n",
    "    b = graph.get_tensor_by_name(\"variables/b:0\").eval()\n",
    "    \n",
    "    print()\n",
    "    print(\"w: %s b: %s \"%(final_w, final_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58B8AIShqriF"
   },
   "source": [
    "### Adding the ops needed for TensorBoard.\n",
    "\n",
    "First, we need to define a log directory. Since we probably want a new log for each run, we conveniently use the date-time of the run as the naming convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FBb9ZDHqriG"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KfDtk_W8qriH"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters (variables). Now organised under the same named scope\n",
    "# It is important to name the variables; this is what will be used in tensorboard\n",
    "with tf.name_scope('variables') as scope:\n",
    "    w = tf.Variable([-3.], dtype=tf.float32, name = 'w')\n",
    "    b = tf.Variable([3.], dtype=tf.float32, name = 'b')\n",
    "\n",
    "# Model input (placeholders)\n",
    "with tf.name_scope('placeholders') as scope:\n",
    "    x = tf.placeholder(tf.float32, name = 'x')\n",
    "    y = tf.placeholder(tf.float32, name = 'y')\n",
    "\n",
    "# Linear model\n",
    "with tf.name_scope('model') as scope:\n",
    "    linear_model = w * x + b\n",
    "    mse = tf.reduce_sum(tf.square(linear_model - y), name = 'mse') \n",
    "    grad = tf.gradients(mse, [w, b], name = 'grad')\n",
    "\n",
    "# Gradient descent update\n",
    "with tf.name_scope('training') as scope:\n",
    "    update_w = tf.assign(w, w - learning_rate * grad[0], name = 'update_w')\n",
    "    update_b = tf.assign(b, b - learning_rate * grad[1], name = 'update_b')\n",
    "\n",
    "\n",
    "# Define init Op\n",
    "with tf.name_scope('init_op') as scope:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "# Define a saver Op that will allow us to save the graph\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "# Execution\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    # 1000 steps\n",
    "    for step in range(1000):\n",
    "        # Report loss every 100 steps\n",
    "        if step%100 == 0:\n",
    "            summary_str = mse_summary.eval(feed_dict={x: x_train, y: y_train})\n",
    "            file_writer.add_summary(summary_str, step)\n",
    "        sess.run([update_w, update_b], {x:x_train, y:y_train})\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKAwQDc7qriJ"
   },
   "source": [
    "If you now go to **tf_logs**, the name of the directoy you created, you should see one or more (depending how many times you ran the code) files of the form, **run-20180520092455**\n",
    "\n",
    "You now want to use TensorBoard. TensorBoard is launched on your command line, **tensorboard --logdir=path/to/log-directory** When is it running, it can be accessed on port 6006 in your browser, i.e. at **localhost:6006**. (Use this link if you get stuck: https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) \n",
    "\n",
    "You should see something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYrsHv_RqriK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Image(\"./tf_logs/tensorboard.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRRQbAbnqriP"
   },
   "source": [
    "# Access to the variables in a graph\n",
    "\n",
    "If you need access to all the ops in a graph, you can do the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVLQo-_rqriQ"
   },
   "outputs": [],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oo0rRuh_qriS"
   },
   "source": [
    "Even for a very simple graph like this one, there are many ops – too many to inspect by hand if the graph gets even a little more involved. To keep track of the ops, in particular those that you may want to access later, there is a better way to do it. This brings us to the next topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjpLqetEqriT"
   },
   "source": [
    "# Saving and restoring a computational graph.\n",
    "\n",
    "You have trained your model and now you want to take it into production. The one thing you do not want to do is to retrain every time you need to query the model. This means that you need to be able to save and restore the model.\n",
    "\n",
    "Note that not only do you want to save the trained variables, but also the computation graph itself so that you don't need to build it from scratch every time you want to use it. This also makes it easy to share your trained model, without needing to provide the code of how you built the model in the first place.\n",
    "\n",
    "The key idea is to add everything necessary to a collection.\n",
    "\n",
    "To illustrate these ideas, we now turn to a more realistic problem, namely digit classification using the MNIST database.\n",
    "\n",
    "The MNIST database consists of the order of 70,000 handwritten digits, divided into training-, validation-, and test-sets of 55,000, 5,000, and 10,000 images, respectively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noegQbVPqriV"
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG0-nEc2qriY"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "m,n = mnist.train.images.shape\n",
    "number_to_show = 100\n",
    "\n",
    "def show_digits(i=0):\n",
    "    \"\"\"\n",
    "    Show some of the digits\n",
    "    \"\"\"\n",
    "    im = np.reshape(mnist.train.images[i], (28,28))\n",
    "    plt.imshow(im, cmap='viridis') \n",
    "    plt.title('The digits')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "w = interact(show_digits, i =(0, number_to_show)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pFuJ1tqJqric"
   },
   "source": [
    "Each image in the training set comes with a label. The idea is to build a classifier from the training data – that is, images and labels that can accurately predict the label of a new image.\n",
    "\n",
    "To keep things simple, we build a straightforward SoftMax classifier. At this point, we are not interested in the accuracy of the system – the idea is to store the trained variables and graph for later use.\n",
    "\n",
    "Much of the code below should already be familiar. Apart from using cross-entropy instead of MSE, a different optimizer, and a few other smaller details, the main novelty is the addition of the final statements where we add the ops that we plan to use later. Note that it is not necessary to add the trained variables to the collection since they are automatically added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LG4iv2bsqrie"
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28   # The size of each image\n",
    "n_outputs = 10     # There are 10 digits, and therefore 10 classes\n",
    "tf.reset_default_graph()\n",
    "\n",
    "stddev = 2/np.sqrt(n_inputs)   \n",
    "\n",
    "with tf.name_scope(\"variable\"):\n",
    "    W = tf.Variable(tf.truncated_normal((784,10), stddev=stddev), name=\"W\")\n",
    "    b = tf.Variable(tf.zeros([10]), name=\"b\")\n",
    "    \n",
    "with tf.name_scope(\"placeholder\"):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y_\") \n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.nn.softmax(tf.matmul(x,W) + b, name=\"logits\")\n",
    "    Y_prob = tf.nn.softmax(logits, name=\"Y_prob\")\n",
    "\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss, name=\"train_op\")\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.equal(tf.argmax(logits,axis=1), tf.argmax(y_,axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    \n",
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()  # We need to add a saver Op\n",
    "\n",
    "# Now we add averything we'll need in future to a collection\n",
    "tf.add_to_collection('train_var', train_op)\n",
    "tf.add_to_collection('train_var', accuracy)\n",
    "tf.add_to_collection('train_var', x)\n",
    "tf.add_to_collection('train_var', y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yqNVWNdqrif"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5ba2624017cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "n_epoch = 100  \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    for epoch in range(n_epoch):   \n",
    "        \n",
    "        # One step of the training\n",
    "        sess.run(train_op, feed_dict={x: mnist.train.images, \n",
    "                                         y_: mnist.train.labels})\n",
    "        \n",
    "        # Evaluate accuracy on the training set\n",
    "        acc_train = accuracy.eval(feed_dict={x: mnist.train.images, \n",
    "                                             y_: mnist.train.labels})\n",
    "        \n",
    "        # Evaluate the accuracy on the test set. This should be \n",
    "        # smaller than the accuracy on the training set\n",
    "        acc_test  = accuracy.eval(feed_dict={x: mnist.test.images,\n",
    "                                             y_: mnist.test.labels})\n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \",  Test accuracy:\", acc_test)   \n",
    "    \n",
    "    # Print the variable names for later access.\n",
    "    var = [v.name for v in tf.trainable_variables()]\n",
    "    print(var) \n",
    "    \n",
    "    weights = graph.get_tensor_by_name(\"variable/W:0\").eval()\n",
    "    bias  = graph.get_tensor_by_name(\"variable/b:0\").eval()\n",
    "    \n",
    "    # Save the current values of the variables, the meta graph, and the collection\n",
    "    # variables to files.\n",
    "    save_path = saver.save(sess,\"./test/test.ckpt\")  \n",
    "    saver.export_meta_graph(filename='./test/test.meta',\n",
    "                            collection_list=[\"train_var\"])\n",
    "# print('weights:', weights)\n",
    "# print('bias:', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CigraEK5qrii"
   },
   "source": [
    "It might be interesting to look at the weights as images. Perhaps you will agree that the weights largely seem to emphasize those regions of the image that best distinguish between the digits. \n",
    "\n",
    "However, this was a bit of a digression – let's continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_vFGdfGqrij"
   },
   "outputs": [],
   "source": [
    "m,n = weights.shape\n",
    "\n",
    "def show_weights(i=0):\n",
    "    im = weights.T[i].reshape([28,28])\n",
    "    plt.imshow(im, cmap='viridis') \n",
    "    plt.title('The weigths of filter '+str(i))\n",
    "    plt.show()\n",
    "    \n",
    "w = interact(show_weights, i =(0, n-1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6IfoqFZqris"
   },
   "source": [
    "# Restoring the variables and the graph.\n",
    "\n",
    "The previous training used only 100 training steps. This is far from the accuracy that can be achieved, even by the basic SoftMax classifier. The task we now set ourselves is to retrieve the graph, as well as the previously trained variables, and continue the training.\n",
    "\n",
    "Crucially, we need to get access to the placeholders in order to feed the training data to the training process.\n",
    "\n",
    "Note that we, as usual, clear the previous graph. We want to make sure we restore the previously saved graph.\n",
    "\n",
    "Also note that we skip the construction phase in its entirety. \n",
    "\n",
    "\n",
    "### Everything we need should be restored from what we saved before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t29YUmQzqrit"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# We do need an Op for restoration!\n",
    "saver = tf.train.import_meta_graph(\"./test/test.ckpt.meta\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # We restore the session and the default graph\n",
    "    # is the previously saved graph\n",
    "    saver.restore(sess,\"./test/test.ckpt\")\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    # Check that we recognise the variables saved before.\n",
    "    var = [v.name for v in tf.trainable_variables()]\n",
    "    print(var)\n",
    "    \n",
    "    # We can retrieve and display the previous weights.\n",
    "    prev_weights = graph.get_tensor_by_name(\"variable/W:0\").eval()\n",
    "    prev_bias  = graph.get_tensor_by_name(\"variable/b:0\").eval()\n",
    "    \n",
    "    # Retrieve all the Ops we need from the collection\n",
    "    train_op = tf.get_collection(\"train_var\")[0]\n",
    "    accuracy = tf.get_collection(\"train_var\")[1]\n",
    "    x = tf.get_collection(\"train_var\")[2]\n",
    "    y_ = tf.get_collection(\"train_var\")[3]\n",
    "    \n",
    "    # Now we simply continue the training\n",
    "    for train_step in range(200):\n",
    "        \n",
    "        sess.run(train_op, feed_dict={x: mnist.train.images, \n",
    "                                      y_: mnist.train.labels})\n",
    "        \n",
    "        if train_step % 10 == 0:\n",
    "            \n",
    "            # Evaluate accuracy on the training set\n",
    "            acc_train = accuracy.eval(feed_dict={x: mnist.train.images, \n",
    "                                             y_: mnist.train.labels})\n",
    "        \n",
    "            # Evaluate the accuracy on the test set. \n",
    "            acc_test  = accuracy.eval(feed_dict={x: mnist.test.images,\n",
    "                                             y_: mnist.test.labels})\n",
    "        \n",
    "            print(train_step, \"Train accuracy:\", acc_train, \",  Test accuracy:\", acc_test)   \n",
    "        \n",
    "        \n",
    "    update_weights = graph.get_tensor_by_name(\"variable/W:0\").eval()\n",
    "    update_bias = graph.get_tensor_by_name(\"variable/b:0\").eval()\n",
    "    \n",
    "    \n",
    "# Checking that we have something useful\n",
    "print(\"Weights:\",weights.shape)\n",
    "print(\"Bias:\",bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egvJ83PZqriy"
   },
   "source": [
    "The accuracy has improved but is actually still poor, by modern standards. Deep neural networks have near-perfect solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NUuzPJZDqriy"
   },
   "outputs": [],
   "source": [
    "m,n = update_weights.shape\n",
    "\n",
    "def show_update_weights(i=0):\n",
    "    \"\"\"\n",
    "    Show the weights\n",
    "    \"\"\"\n",
    "    im = update_weights.T[i].reshape([28,28])\n",
    "    plt.imshow(im, cmap='viridis') \n",
    "    plt.title('The weigths of filter '+str(i))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "w = interact(show_update_weights, i =(0, n-1)) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "I. TensorFlow_Basics.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
